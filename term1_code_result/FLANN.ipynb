{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khanh\\Anaconda3\\envs\\py35\\lib\\site-packages\\matplotlib\\cbook.py:136: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import helper\n",
    "import datetime\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from matplotlib import style\n",
    "import yahoo_finance as yahoo\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>DJIA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date.1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>112.4375</td>\n",
       "      <td>116.0000</td>\n",
       "      <td>11357.509766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>114.0000</td>\n",
       "      <td>112.0625</td>\n",
       "      <td>10997.929688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>112.9375</td>\n",
       "      <td>116.0000</td>\n",
       "      <td>11122.650391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>118.0000</td>\n",
       "      <td>114.0000</td>\n",
       "      <td>11253.259766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>117.2500</td>\n",
       "      <td>113.5000</td>\n",
       "      <td>11522.559570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open     Close          DJIA\n",
       "Date.1                                      \n",
       "2000-01-03  112.4375  116.0000  11357.509766\n",
       "2000-01-04  114.0000  112.0625  10997.929688\n",
       "2000-01-05  112.9375  116.0000  11122.650391\n",
       "2000-01-06  118.0000  114.0000  11253.259766\n",
       "2000-01-07  117.2500  113.5000  11522.559570"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/ibm/full_ibm.csv')\n",
    "df['Date.1'] = pd.to_datetime(df['Date.1'])\n",
    "df.set_index('Date.1', inplace=True)\n",
    "df.drop(['Date','High', 'Adj Close', 'NDX', 'SP500', 'Volume', 'Low'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05', '2000-01-06',\n",
       "               '2000-01-07', '2000-01-10', '2000-01-11', '2000-01-12',\n",
       "               '2000-01-13', '2000-01-14',\n",
       "               ...\n",
       "               '2016-12-16', '2016-12-19', '2016-12-20', '2016-12-21',\n",
       "               '2016-12-22', '2016-12-23', '2016-12-27', '2016-12-28',\n",
       "               '2016-12-29', '2016-12-30'],\n",
       "              dtype='datetime64[ns]', name='Date.1', length=4277, freq=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>DJIA</th>\n",
       "      <th>ma7</th>\n",
       "      <th>ma30</th>\n",
       "      <th>std30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date.1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-23</th>\n",
       "      <td>167.000000</td>\n",
       "      <td>166.710007</td>\n",
       "      <td>19933.810547</td>\n",
       "      <td>167.161429</td>\n",
       "      <td>163.586334</td>\n",
       "      <td>3.291480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>166.979996</td>\n",
       "      <td>167.139999</td>\n",
       "      <td>19945.039062</td>\n",
       "      <td>167.035714</td>\n",
       "      <td>163.782000</td>\n",
       "      <td>3.323355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>167.289993</td>\n",
       "      <td>166.190002</td>\n",
       "      <td>19833.679688</td>\n",
       "      <td>166.958572</td>\n",
       "      <td>164.048000</td>\n",
       "      <td>3.178183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>166.020004</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>19819.779297</td>\n",
       "      <td>166.947146</td>\n",
       "      <td>164.312334</td>\n",
       "      <td>3.042335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>166.440002</td>\n",
       "      <td>165.990005</td>\n",
       "      <td>19762.599609</td>\n",
       "      <td>166.717146</td>\n",
       "      <td>164.535667</td>\n",
       "      <td>2.903699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open       Close          DJIA         ma7        ma30  \\\n",
       "Date.1                                                                     \n",
       "2016-12-23  167.000000  166.710007  19933.810547  167.161429  163.586334   \n",
       "2016-12-27  166.979996  167.139999  19945.039062  167.035714  163.782000   \n",
       "2016-12-28  167.289993  166.190002  19833.679688  166.958572  164.048000   \n",
       "2016-12-29  166.020004  166.600006  19819.779297  166.947146  164.312334   \n",
       "2016-12-30  166.440002  165.990005  19762.599609  166.717146  164.535667   \n",
       "\n",
       "               std30  \n",
       "Date.1                \n",
       "2016-12-23  3.291480  \n",
       "2016-12-27  3.323355  \n",
       "2016-12-28  3.178183  \n",
       "2016-12-29  3.042335  \n",
       "2016-12-30  2.903699  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ma7'] = df['Close'].rolling(window=7).mean()\n",
    "df['ma30'] = df['Close'].rolling(window=30).mean()\n",
    "df['std30'] = df['Close'].rolling(window=30).std()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4277, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>DJIA</th>\n",
       "      <th>std30</th>\n",
       "      <th>ma7_1</th>\n",
       "      <th>ma7_2</th>\n",
       "      <th>ma7_3</th>\n",
       "      <th>ma7_4</th>\n",
       "      <th>ma30_1</th>\n",
       "      <th>ma30_2</th>\n",
       "      <th>ma30_3</th>\n",
       "      <th>ma30_4</th>\n",
       "      <th>Close_1</th>\n",
       "      <th>Close_2</th>\n",
       "      <th>Close_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date.1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-23</th>\n",
       "      <td>167.000000</td>\n",
       "      <td>19933.810547</td>\n",
       "      <td>3.291480</td>\n",
       "      <td>161.771430</td>\n",
       "      <td>162.714286</td>\n",
       "      <td>158.895715</td>\n",
       "      <td>153.445711</td>\n",
       "      <td>155.241427</td>\n",
       "      <td>162.392857</td>\n",
       "      <td>157.824286</td>\n",
       "      <td>152.801428</td>\n",
       "      <td>167.059998</td>\n",
       "      <td>167.330002</td>\n",
       "      <td>167.600006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>166.979996</td>\n",
       "      <td>19945.039062</td>\n",
       "      <td>3.323355</td>\n",
       "      <td>162.385716</td>\n",
       "      <td>162.975714</td>\n",
       "      <td>159.692858</td>\n",
       "      <td>153.605711</td>\n",
       "      <td>155.729998</td>\n",
       "      <td>161.992857</td>\n",
       "      <td>158.555714</td>\n",
       "      <td>152.949999</td>\n",
       "      <td>166.710007</td>\n",
       "      <td>167.059998</td>\n",
       "      <td>167.330002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>167.289993</td>\n",
       "      <td>19833.679688</td>\n",
       "      <td>3.178183</td>\n",
       "      <td>163.197143</td>\n",
       "      <td>162.554286</td>\n",
       "      <td>160.057144</td>\n",
       "      <td>154.667140</td>\n",
       "      <td>156.252856</td>\n",
       "      <td>161.764287</td>\n",
       "      <td>159.417143</td>\n",
       "      <td>153.077142</td>\n",
       "      <td>167.139999</td>\n",
       "      <td>166.710007</td>\n",
       "      <td>167.059998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>166.020004</td>\n",
       "      <td>19819.779297</td>\n",
       "      <td>3.042335</td>\n",
       "      <td>164.378570</td>\n",
       "      <td>162.175716</td>\n",
       "      <td>160.257143</td>\n",
       "      <td>155.998570</td>\n",
       "      <td>156.727142</td>\n",
       "      <td>161.705715</td>\n",
       "      <td>159.904286</td>\n",
       "      <td>153.242857</td>\n",
       "      <td>166.190002</td>\n",
       "      <td>167.139999</td>\n",
       "      <td>166.710007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>166.440002</td>\n",
       "      <td>19762.599609</td>\n",
       "      <td>2.903699</td>\n",
       "      <td>165.617142</td>\n",
       "      <td>161.870001</td>\n",
       "      <td>160.795713</td>\n",
       "      <td>156.832857</td>\n",
       "      <td>156.941428</td>\n",
       "      <td>161.414285</td>\n",
       "      <td>160.482858</td>\n",
       "      <td>153.197141</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>166.190002</td>\n",
       "      <td>167.139999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open          DJIA     std30       ma7_1       ma7_2  \\\n",
       "Date.1                                                                   \n",
       "2016-12-23  167.000000  19933.810547  3.291480  161.771430  162.714286   \n",
       "2016-12-27  166.979996  19945.039062  3.323355  162.385716  162.975714   \n",
       "2016-12-28  167.289993  19833.679688  3.178183  163.197143  162.554286   \n",
       "2016-12-29  166.020004  19819.779297  3.042335  164.378570  162.175716   \n",
       "2016-12-30  166.440002  19762.599609  2.903699  165.617142  161.870001   \n",
       "\n",
       "                 ma7_3       ma7_4      ma30_1      ma30_2      ma30_3  \\\n",
       "Date.1                                                                   \n",
       "2016-12-23  158.895715  153.445711  155.241427  162.392857  157.824286   \n",
       "2016-12-27  159.692858  153.605711  155.729998  161.992857  158.555714   \n",
       "2016-12-28  160.057144  154.667140  156.252856  161.764287  159.417143   \n",
       "2016-12-29  160.257143  155.998570  156.727142  161.705715  159.904286   \n",
       "2016-12-30  160.795713  156.832857  156.941428  161.414285  160.482858   \n",
       "\n",
       "                ma30_4     Close_1     Close_2     Close_3  \n",
       "Date.1                                                      \n",
       "2016-12-23  152.801428  167.059998  167.330002  167.600006  \n",
       "2016-12-27  152.949999  166.710007  167.059998  167.330002  \n",
       "2016-12-28  153.077142  167.139999  166.710007  167.059998  \n",
       "2016-12-29  153.242857  166.190002  167.139999  166.710007  \n",
       "2016-12-30  153.197141  166.600006  166.190002  167.139999  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.drop(['ma7', 'Close', 'ma30'],axis=1)\n",
    "df_train['ma7_1'] = df['ma7'].shift(periods = 11, axis=0)\n",
    "df_train['ma7_2'] = df['ma7'].shift(periods = 18, axis=0)\n",
    "df_train['ma7_3'] = df['ma7'].shift(periods = 25, axis=0)\n",
    "df_train['ma7_4'] = df['ma7'].shift(periods = 32, axis=0)\n",
    "df_train['ma30_1'] = df['ma7'].shift(periods = 62, axis=0)\n",
    "df_train['ma30_2'] = df['ma7'].shift(periods = 92, axis=0)\n",
    "df_train['ma30_3'] = df['ma7'].shift(periods = 112, axis=0)\n",
    "df_train['ma30_4'] = df['ma7'].shift(periods = 142, axis=0)\n",
    "df_train['Close_1'] = df['Close'].shift(periods = 1, axis=0)\n",
    "df_train['Close_2'] = df['Close'].shift(periods = 2, axis=0)\n",
    "df_train['Close_3'] = df['Close'].shift(periods = 3, axis=0)\n",
    "print (df_train.shape)\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## params\n",
    "N_FEATURES = 126 # number of input features\n",
    "X = tf.placeholder(tf.float32, [None, N_FEATURES])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Tanh:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def new_fc_layer(in_layer, n_input_features, n_output_layer, activation=''):\n",
    "    weights = tf.Variable(tf.truncated_normal([n_input_features, n_output_layer], stddev=0.1))\n",
    "    biases = tf.Variable(tf.constant(value=1, dtype=tf.float32, shape = [n_output_layer]))\n",
    "    out_layer = tf.matmul(in_layer, weights) + biases\n",
    "    activation = activation.lower()\n",
    "    if(activation == 'relu'):\n",
    "        out_layer = tf.nn.relu(out_layer)\n",
    "    elif(activation == 'tanh'):\n",
    "        out_layer = tf.nn.tanh(out_layer)\n",
    "    elif(activation == 'sigmoid'):\n",
    "        out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer, weights, biases\n",
    "## Hidden layer 1\n",
    "N_HIDDEN_L1 = 1\n",
    "hidden_layer_1, weight_1, biases_1 = new_fc_layer(X, N_FEATURES, N_HIDDEN_L1, 'tanh')\n",
    "\n",
    "print (hidden_layer_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "def decay_learning_rate(learning_step, total_step):\n",
    "    ini = 1e-3\n",
    "    if(learning_step <= 3000):\n",
    "        return ini\n",
    "    return ini*(1-(learning_step-3000)/total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_stock(index, x1, x2, dir_path, rmse, mae, epoch, plot, train):\n",
    "    if(train):\n",
    "        folder = dir_path + 'train/'\n",
    "    else:\n",
    "        folder = dir_path + 'test/'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    years = mdates.YearLocator()   # every year\n",
    "    months = mdates.MonthLocator()  # every month\n",
    "    yearsFmt = mdates.DateFormatter('%Y')\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    if(plot=='abs'):\n",
    "        abs_error = np.abs(x1 - x2)\n",
    "        ax.plot(index, abs_error, color = 'b', label='Absolute error')\n",
    "        ax.text(index[1], abs_error.max() - 0.01, \"MAE={:7.5f}\".format(mae), style='italic',\n",
    "            bbox={'facecolor':'blue', 'alpha':0.5, 'pad':10})\n",
    "    elif(plot=='cost'):\n",
    "        ax.plot(index, x1, color = 'b', label='Cost value')\n",
    "    elif(plot=='rmse'):\n",
    "        ax.plot(index, x1, color = 'b', label='RMSE value')\n",
    "    elif(plot=='prediction'):  \n",
    "        ax.plot(index, x1, color = 'b', label='Acutal Values')\n",
    "        ax.plot(index, x2, color = 'r', label='Predicted Values')\n",
    "        ax.text(index[1], x1.min() + 0.1, \"RMSE={:7.5f}\".format(rmse), style='italic',\n",
    "            bbox={'facecolor':'blue', 'alpha':0.5, 'pad':10})\n",
    "    # format the ticks\n",
    "    ax.xaxis.set_major_locator(years)\n",
    "    ax.xaxis.set_major_formatter(yearsFmt)\n",
    "    ax.xaxis.set_minor_locator(months)\n",
    "    datemin = datetime.date(index.date.min().year, 1, 1)\n",
    "    datemax = datetime.date(index.date.max().year + 1, 1, 1)\n",
    "    ax.set_xlim(datemin, datemax)\n",
    "    ax.format_xdata = mdates.DateFormatter('%d-%m-%Y')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('norm price')\n",
    "    ax.legend(loc='upper right', shadow=False)\n",
    "    ax.grid(True)\n",
    "    fig.autofmt_xdate()\n",
    "     \n",
    "    if(plot == 'abs'):\n",
    "        plt.savefig('{0}Abs_error_{1}th_epoch'.format(folder, epoch), dpi=1000)\n",
    "    elif(plot =='cost'):\n",
    "        plt.savefig('{0}cost_value'.format(folder), dpi=1000)\n",
    "    elif(plot=='rmse'):\n",
    "        plt.savefig('{0}rmse_value'.format(folder), dpi=1000)\n",
    "    elif(plot=='prediction'):\n",
    "        plt.savefig('{0}{1}th_epoch'.format(folder, epoch), dpi=1000)\n",
    "    plt.clf()\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_metrics(true, pred):\n",
    "    rmse = math.sqrt(metrics.mean_squared_error(pred, true))\n",
    "    mae = metrics.mean_absolute_error(pred, true)\n",
    "    hit = np.mean(helper.step((pred[1:] - true[:-1])*(true[1:] - true[:-1])))\n",
    "    return rmse, mae, hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "TRAINING_EPOCHS = 10000\n",
    "SAMPLING_RATE = 500\n",
    "## Helper function for optimization\n",
    "def optimize(train_x, train_y, test_x, test_y, n_epochs, batch_size, session, saver, dir_path, n_ensemble):\n",
    "    n_samples = train_x.shape[0]\n",
    "    n_iterations = np.int(np.floor(n_samples/batch_size))+1\n",
    "    start_time = time.time()\n",
    "    cost_history = np.empty(shape=[1],dtype=float)\n",
    "    train_metrics = np.zeros(shape=(np.int(n_epochs // SAMPLING_RATE)+1,3), dtype=float)\n",
    "    test_metrics = np.zeros(shape=(np.int(n_epochs // SAMPLING_RATE)+1,3), dtype=float)\n",
    "    train_prediction_record = np.zeros(shape=(np.int(n_epochs // SAMPLING_RATE)+1,train_y.shape[0], train_y.shape[1]))\n",
    "    test_prediction_record = np.zeros(shape=(np.int(n_epochs // SAMPLING_RATE)+1,test_y.shape[0], test_y.shape[1]))\n",
    "    train_y_matrix = train_y.as_matrix()\n",
    "    best_iteration = 0\n",
    "    previous_mse = 1000000\n",
    "    print (\"Training.......\")\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    model_path = \"{0}model_{1}/\".format(dir_path, n_ensemble)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    for epoch in np.arange(n_epochs+1):\n",
    "        for itr in np.arange(n_iterations):\n",
    "            start = (itr * batch_size) % (n_samples )\n",
    "            batch_x, batch_y = train_x[start:start + batch_size], train_y_matrix[start:start + batch_size]\n",
    "            lr = decay_learning_rate(epoch, n_epochs)\n",
    "            feed_dict_train = {X: batch_x, Y: batch_y, learning_rate: lr}\n",
    "            _, c = session.run([optimizer, cost], feed_dict=feed_dict_train)\n",
    "        prediction = make_prediction(test_x, test_y, session, BATCH_SIZE)\n",
    "        r, _, _ = cal_metrics(test_y.as_matrix(), prediction)\n",
    "        if(r < previous_mse):\n",
    "            saver.save(session, \"{}model.ckpt\".format(model_path))\n",
    "            previous_mse = r \n",
    "            best_iteration = epoch\n",
    "        if(epoch >= SAMPLING_RATE):\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        \n",
    "            \n",
    "        if(epoch % SAMPLING_RATE == 0):\n",
    "            counter = np.int(epoch // SAMPLING_RATE)\n",
    "\n",
    "    \n",
    "            test_prediction_record[counter] = prediction\n",
    "            \n",
    "#             print (\"-- Elapsed time -- Epoch -- Cost value -- \")\n",
    "#             print (\"-- {:12.6f} -- {:5d} -- {:10.5f} -- \".format((time.time() - start_time), \n",
    "#                                                                                     epoch, \n",
    "#                                                                                     c, \n",
    "#                                                                                     ))\n",
    "#             print (\"-- Making training prediction at {}th epoch\".format(epoch))\n",
    "            prediction = make_prediction(train_x, train_y, session, BATCH_SIZE)  \n",
    "            \n",
    "            train_prediction_record[counter] = prediction \n",
    "#             print (\"-- Making testing prediction at {}th epoch\".format(epoch))\n",
    "            \n",
    "        \n",
    "#             Draw weights of convolutional layer\n",
    "#             if(epoch % (n_epochs/2) == 0):\n",
    "#                 plot_conv_weights(session, conv_weights[0], 'conv_1', 1, epoch)\n",
    "#                 plot_conv_weights(session, conv_weights[1], 'conv_2', 1, epoch)\n",
    "#                 plot_conv_weights(session, conv_weights[2], 'conv_3', 1, epoch)\n",
    "                  \n",
    "#         Save model in folder rnn_model\n",
    "        \n",
    "#         saver.save(sess, 'rnn_model/new_cnn')\n",
    "        \n",
    "#         print running time and output cost value graph\n",
    "    running_time = time.time() - start_time\n",
    "    print (\"---Total Running time: %s seconds ---\" % (running_time))\n",
    "    print ('*'*50)\n",
    "    return train_prediction_record, test_prediction_record, running_time, best_iteration \n",
    "    \n",
    "    \n",
    "## Helper function to print confusion matrix\n",
    "def make_prediction(test_x, test_y, sess, batch_size):\n",
    "    start_time = time.time()\n",
    "    n_samples = test_x.shape[0]\n",
    "    n_iterations = np.int(np.floor(n_samples/batch_size))+1 \n",
    "    pred = np.zeros((n_samples, test_y.shape[1]))\n",
    "#     true = test_y.as_matrix()\n",
    "    for itr in np.arange(n_iterations):\n",
    "        start = (itr * batch_size) % (n_samples)\n",
    "        batch_x = test_x[start:start + batch_size]\n",
    "        feed_dict_test = {X: batch_x}\n",
    "        pred[start:start + batch_size] = sess.run(y_pred, feed_dict=feed_dict_test)\n",
    "#     pred = (pred + 1) * (true.max() - true.min()) / 2 + true.min()\n",
    "#     rmse = math.sqrt(metrics.mean_squared_error(pred, true))\n",
    "#     mae = metrics.mean_absolute_error(pred, true)\n",
    "#     hit = np.mean(helper.step((pred[1:] - true[:-1])*(true[1:] - true[:-1])))\n",
    "#     print (\"RMSE = {}\".format(rmse))\n",
    "#     print (\"MAE = {}\".format(mae))\n",
    "#     print (\"Hit rate = {}\".format(hit))\n",
    "#     print ('*'*50)\n",
    "#     return rmse, mae, hit, pred \n",
    "    return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trignometric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_ensemble(n_ensemble, train_x, train_y, test_x, test_y, n_epochs, batch_size, d):\n",
    "    \n",
    "    n = np.int(n_epochs // SAMPLING_RATE)+1\n",
    "    train_pred = np.zeros(shape=(n,train_y.shape[0], train_y.shape[1]))\n",
    "    test_pred = np.zeros(shape=(n,test_y.shape[0], test_y.shape[1]))\n",
    "    train_metrics = np.zeros(shape=(n,3), dtype=float)\n",
    "    test_metrics = np.zeros(shape=(n,3), dtype=float)\n",
    "    running_times = np.zeros(shape=(n_ensemble,1), dtype=float)\n",
    "    best_iteration = np.zeros(shape=(n_ensemble,1), dtype=int)\n",
    "    best_pred = np.zeros(shape=test_y.shape)\n",
    "    pred_time = 0\n",
    "    for i in range(n_ensemble):\n",
    "        print ('='*50)\n",
    "        print (\"Esemble {}\".format(i))\n",
    "        saver = tf.train.Saver()\n",
    "        temp_train_pred = np.zeros(shape=(n,train_y.shape[0], train_y.shape[1]))\n",
    "        temp_test_pred = np.zeros(shape=(n,test_y.shape[0], test_y.shape[1]))\n",
    "#         temp_train_metrics = np.zeros(shape=(n,3), dtype=float)\n",
    "#         temp_test_metrics = np.zeros(shape=(n,3), dtype=float)\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            temp_train_pred,temp_test_pred, running_times[i,0], best_iteration[i,0] = optimize(x_train, y_train, x_test, y_test , n_epochs, batch_size, sess, saver, d, i)\n",
    "#         train_metrics = train_metrics + (temp_train_metrics - train_metrics)/(i+1)\n",
    "#         test_metrics = test_metrics + (temp_test_metrics - test_metrics)/(i+1)\n",
    "#         temp_train_metrics[i, 0]\n",
    "        train_pred = train_pred + (temp_train_pred - train_pred)/(i+1)\n",
    "        test_pred = test_pred + (temp_test_pred - test_pred)/(i+1)\n",
    "    for i in range(n):\n",
    "        train_metrics[i,0], train_metrics[i,1], train_metrics[i,2] = cal_metrics(train_y.as_matrix(), train_pred[i])\n",
    "        test_metrics[i,0], test_metrics[i,1], test_metrics[i,2] = cal_metrics(test_y.as_matrix(), test_pred[i])\n",
    "    for i in np.arange(0, n, 2):\n",
    "        plot_stock(train_y.index, train_y, train_pred[i], d, train_metrics[i,0], train_metrics[i,1], i*SAMPLING_RATE, 'prediction', True)\n",
    "        plot_stock(train_y.index, train_y, train_pred[i], d, train_metrics[i,0], train_metrics[i,1], i*SAMPLING_RATE, 'abs', True)\n",
    "        plot_stock(test_y.index, test_y, test_pred[i], d, test_metrics[i,0], test_metrics[i,1], i*SAMPLING_RATE, 'prediction', False)\n",
    "        plot_stock(test_y.index, test_y, test_pred[i], d, test_metrics[i,0], test_metrics[i,1], i*SAMPLING_RATE, 'abs', False)\n",
    "    train_metrics_df = pd.DataFrame(train_metrics, index=np.arange(0,n_epochs+1,SAMPLING_RATE),columns=['RMSE','MAE','HITRATE'])\n",
    "    train_metrics_df.to_csv(d+'train/metrics.csv')\n",
    "    test_metrics_df = pd.DataFrame(test_metrics, index=np.arange(0,n_epochs+1,SAMPLING_RATE),columns=['RMSE','MAE','HITRATE'])\n",
    "    test_metrics_df.to_csv(d+'test/metrics.csv')\n",
    "    \n",
    "    for i in range(n_ensemble):\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, \"{0}model_{1}/model.ckpt\".format(d, i))\n",
    "            sta = time.time()\n",
    "            prediction = make_prediction(test_x, test_y, sess, BATCH_SIZE)\n",
    "            run = time.time() - sta\n",
    "            pred_time = pred_time + (run - pred_time)/(i+1)\n",
    "            r, m, h = cal_metrics(test_y.as_matrix(), prediction)\n",
    "            plot_stock(test_y.index, test_y, prediction, d, r, m, best_iteration[i,0], 'prediction', False)\n",
    "            plot_stock(test_y.index, y_test, prediction, d, r, m, best_iteration[i,0], 'abs', False)\n",
    "            best_pred = best_pred + (prediction - best_pred)/(i+1)\n",
    "    rmse, mae, hit = cal_metrics(test_y.as_matrix(), best_pred)\n",
    "    print (\"Prediction time = {}\".format(pred_time))\n",
    "    print (\" *** RMSE={:7.5f}, MAE={:7.5f}, HIT={:7.5f} ***\".format(rmse, mae, hit))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_d = helper.inter_polation_norm_1_1(df_train)\n",
    "n_ensembles = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "# def decay_learning_rate(learning_step, total_step):\n",
    "#     ini = 1e-3\n",
    "#     if(learning_step <= 3000):\n",
    "#         return ini\n",
    "#     return ini*(1-(learning_step-3000)/total_step)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.square(hidden_layer_1-Y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "# y_pred = hidden_layer_1\n",
    "# y_true = Y\n",
    "# ## Trigonometric expansion\n",
    "# df_train_data = helper.trignometric_expansion(df_train_d, 4)\n",
    "# df_target = df.loc[df_train_data.index][['Close']]\n",
    "# df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "# d = './FLANN/tri_decay/'\n",
    "# ## Train test split\n",
    "# x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "# x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "# train_ensemble(10, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 511.54350757598877 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 519.9518010616302 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 517.5722782611847 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 514.9747905731201 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 468.4525315761566 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 503.239946603775 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 503.8340001106262 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 579.4184858798981 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 533.1149163246155 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 533.7855937480927 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay/model_9/model.ckpt\n",
      "Prediction time = 0.008166360855102538\n",
      " *** RMSE=0.04341, MAE=0.03197, HIT=0.57748 ***\n",
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 634.3605444431305 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 577.2939896583557 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 542.6736161708832 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 528.2927975654602 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 632.342476606369 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 485.1425862312317 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 610.276294708252 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 650.5216228961945 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 619.1165475845337 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 623.7813036441803 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay/model_9/model.ckpt\n",
      "Prediction time = 0.009632515907287595\n",
      " *** RMSE=0.05956, MAE=0.04170, HIT=0.55497 ***\n",
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 526.0478186607361 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 531.4539580345154 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 569.2719831466675 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 557.3211259841919 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 580.140157699585 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 562.9492914676666 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 600.7957277297974 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 623.8829076290131 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 585.5072152614594 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 561.463036775589 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay/model_9/model.ckpt\n",
      "Prediction time = 0.012050867080688477\n",
      " *** RMSE=0.16183, MAE=0.12597, HIT=0.51656 ***\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.square(hidden_layer_1-Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "y_pred = hidden_layer_1\n",
    "y_true = Y\n",
    "df_train_d = helper.inter_polation_norm_1_1(df_train)\n",
    "df_train_data = helper.trignometric_expansion(df_train_d, 4)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "d = './FLANN/tri_decay/'\n",
    "## Train test split\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)\n",
    "\n",
    "\n",
    "df_train_data = helper.chebyshev_expansion(df_train_d, 8)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "## Train test split\n",
    "d = './FLANN/che_decay/'\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)\n",
    "\n",
    "df_train_data = helper.legendre_expansion(df_train_d, 8)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "## Train test split\n",
    "d = './FLANN/leg_decay/'\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 985.2641668319702 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 902.734424829483 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 872.7211511135101 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 926.3221204280853 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 945.5492050647736 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 801.8306787014008 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 893.1008937358856 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 1020.1596300601959 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 971.3183348178864 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 873.2210083007812 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_decay_L2/model_9/model.ckpt\n",
      "Prediction time = 0.020464563369750974\n",
      " *** RMSE=0.04986, MAE=0.03794, HIT=0.51391 ***\n",
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 1076.6032755374908 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 999.707332611084 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 1234.950828552246 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 1310.2799224853516 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 1326.9304819107056 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 1201.6575322151184 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 1288.2112653255463 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 1404.184489250183 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 1458.0269322395325 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 1424.4868695735931 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_decay_L2/model_9/model.ckpt\n",
      "Prediction time = 0.020737338066101074\n",
      " *** RMSE=0.03353, MAE=0.02645, HIT=0.54172 ***\n",
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 1136.9074475765228 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 1290.3812019824982 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 1322.5606529712677 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 1146.3179223537445 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 1260.3534288406372 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 1131.6907839775085 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 1272.923006772995 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 1257.4206204414368 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 1381.4738247394562 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 1339.8518903255463 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_decay_L2/model_9/model.ckpt\n",
      "Prediction time = 0.025730013847351074\n",
      " *** RMSE=0.03103, MAE=0.02415, HIT=0.56026 ***\n"
     ]
    }
   ],
   "source": [
    "beta = 0.0001\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "def decay_learning_rate(learning_step, total_step):\n",
    "    ini = 1e-3\n",
    "    if(learning_step <= 3000):\n",
    "        return ini\n",
    "    return ini*(1-(learning_step-3000)/total_step)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hidden_layer_1-Y)) + beta*tf.nn.l2_loss(weight_1)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "y_pred = hidden_layer_1\n",
    "y_true = Y\n",
    "## Trigonometric expansion\n",
    "df_train_data = helper.trignometric_expansion(df_train_d, 4)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "d = './FLANN/tri_decay_L2/'\n",
    "## Train test split\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)\n",
    "\n",
    "## Chebyshev expansion\n",
    "df_train_data = helper.chebyshev_expansion(df_train_d, 8)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "## Train test split\n",
    "d = './FLANN/che_decay_L2/'\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)\n",
    "\n",
    "## Legendre expansion\n",
    "df_train_data = helper.legendre_expansion(df_train_d, 8)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "## Train test split\n",
    "d = './FLANN/leg_decay_L2/'\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 1413.8336727619171 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 2287.465895175934 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 1782.713847875595 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 1841.846482515335 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 1624.6334261894226 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 2330.9864428043365 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 1901.4000971317291 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 2691.6738991737366 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 1701.868626832962 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 2374.459159374237 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/tri_fix/model_9/model.ckpt\n",
      "Prediction time = 0.031374764442443845\n",
      " *** RMSE=0.06042, MAE=0.04968, HIT=0.51258 ***\n",
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 2303.1627259254456 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 2100.6474936008453 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 2281.2084012031555 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 2115.059777736664 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 1490.7548005580902 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 2021.1682467460632 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 2591.416302919388 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 1542.386002779007 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 2844.9226508140564 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 3395.139020681381 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/che_fix/model_9/model.ckpt\n",
      "Prediction time = 0.04283053874969483\n",
      " *** RMSE=0.07617, MAE=0.06212, HIT=0.49536 ***\n",
      "==================================================\n",
      "Esemble 0\n",
      "Training.......\n",
      "---Total Running time: 3981.498238325119 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 1\n",
      "Training.......\n",
      "---Total Running time: 4090.46400642395 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 2\n",
      "Training.......\n",
      "---Total Running time: 4214.544209480286 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 3\n",
      "Training.......\n",
      "---Total Running time: 4304.231246948242 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 4\n",
      "Training.......\n",
      "---Total Running time: 3359.9415860176086 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 5\n",
      "Training.......\n",
      "---Total Running time: 4354.981750011444 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 6\n",
      "Training.......\n",
      "---Total Running time: 4426.604765415192 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 7\n",
      "Training.......\n",
      "---Total Running time: 4326.514537096024 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 8\n",
      "Training.......\n",
      "---Total Running time: 3694.9712991714478 seconds ---\n",
      "**************************************************\n",
      "==================================================\n",
      "Esemble 9\n",
      "Training.......\n",
      "---Total Running time: 3895.9526178836823 seconds ---\n",
      "**************************************************\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_0/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_1/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_2/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_3/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_4/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_5/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_6/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_7/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_8/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./FLANN/leg_fix/model_9/model.ckpt\n",
      "Prediction time = 0.0439032793045044\n",
      " *** RMSE=0.09309, MAE=0.06009, HIT=0.54967 ***\n"
     ]
    }
   ],
   "source": [
    "TRAINING_EPOCHS = 20000\n",
    "SAMPLING_RATE = 500\n",
    "## Cost function and optimization\n",
    "# BETA = 0.001 #L2 regularization penalty factor\n",
    "LEARNING_RATE = 1e-3\n",
    "cost = tf.reduce_mean(tf.square(hidden_layer_1-Y)) # To be added L2 regularization\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost)\n",
    "## Making Prediction\n",
    "y_pred = hidden_layer_1\n",
    "y_true = Y\n",
    "df_train_d = helper.inter_polation_norm_1_1(df_train)\n",
    "df_train_data = helper.trignometric_expansion(df_train_d, 4)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "d = './FLANN/tri_fix/'\n",
    "## Train test split\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)\n",
    "\n",
    "\n",
    "df_train_data = helper.chebyshev_expansion(df_train_d, 8)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "## Train test split\n",
    "d = './FLANN/che_fix/'\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)\n",
    "\n",
    "df_train_data = helper.legendre_expansion(df_train_d, 8)\n",
    "df_target = df.loc[df_train_data.index][['Close']]\n",
    "df_target = helper.inter_polation_norm_1_1(df_target)\n",
    "## Train test split\n",
    "d = './FLANN/leg_fix/'\n",
    "x_train, y_train = df_train_data.loc[:'2014-01-01'].as_matrix(), df_target.loc[:'2014-01-01']\n",
    "x_test, y_test  = df_train_data.loc['2014-01-01':].as_matrix(), df_target.loc['2014-01-01':]\n",
    "train_ensemble(n_ensembles, x_train, y_train, x_test, y_test , TRAINING_EPOCHS, BATCH_SIZE, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
