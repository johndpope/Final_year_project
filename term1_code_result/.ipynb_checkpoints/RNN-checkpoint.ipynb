{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khanh\\Anaconda3\\envs\\py35\\lib\\site-packages\\matplotlib\\cbook.py:136: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from matplotlib import style\n",
    "import yahoo_finance as yahoo\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.finance import candlestick_ohlc\n",
    "from IPython.display import display, Math, Latex\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>NDX</th>\n",
       "      <th>DJIA</th>\n",
       "      <th>SP500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date.1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>112.4375</td>\n",
       "      <td>116.0000</td>\n",
       "      <td>111.875</td>\n",
       "      <td>116.0000</td>\n",
       "      <td>10347700</td>\n",
       "      <td>87.761136</td>\n",
       "      <td>3790.550049</td>\n",
       "      <td>11357.509766</td>\n",
       "      <td>1455.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>114.0000</td>\n",
       "      <td>114.5000</td>\n",
       "      <td>110.875</td>\n",
       "      <td>112.0625</td>\n",
       "      <td>8227800</td>\n",
       "      <td>84.782175</td>\n",
       "      <td>3546.199951</td>\n",
       "      <td>10997.929688</td>\n",
       "      <td>1399.420044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>112.9375</td>\n",
       "      <td>119.7500</td>\n",
       "      <td>112.125</td>\n",
       "      <td>116.0000</td>\n",
       "      <td>12733200</td>\n",
       "      <td>87.761136</td>\n",
       "      <td>3507.310059</td>\n",
       "      <td>11122.650391</td>\n",
       "      <td>1402.109985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>118.0000</td>\n",
       "      <td>118.9375</td>\n",
       "      <td>113.500</td>\n",
       "      <td>114.0000</td>\n",
       "      <td>7971900</td>\n",
       "      <td>86.248013</td>\n",
       "      <td>3340.810059</td>\n",
       "      <td>11253.259766</td>\n",
       "      <td>1403.449951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>117.2500</td>\n",
       "      <td>117.9375</td>\n",
       "      <td>110.625</td>\n",
       "      <td>113.5000</td>\n",
       "      <td>11856700</td>\n",
       "      <td>85.869732</td>\n",
       "      <td>3529.600098</td>\n",
       "      <td>11522.559570</td>\n",
       "      <td>1441.469971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High      Low     Close    Volume  Adj Close  \\\n",
       "Date.1                                                                   \n",
       "2000-01-03  112.4375  116.0000  111.875  116.0000  10347700  87.761136   \n",
       "2000-01-04  114.0000  114.5000  110.875  112.0625   8227800  84.782175   \n",
       "2000-01-05  112.9375  119.7500  112.125  116.0000  12733200  87.761136   \n",
       "2000-01-06  118.0000  118.9375  113.500  114.0000   7971900  86.248013   \n",
       "2000-01-07  117.2500  117.9375  110.625  113.5000  11856700  85.869732   \n",
       "\n",
       "                    NDX          DJIA        SP500  \n",
       "Date.1                                              \n",
       "2000-01-03  3790.550049  11357.509766  1455.219971  \n",
       "2000-01-04  3546.199951  10997.929688  1399.420044  \n",
       "2000-01-05  3507.310059  11122.650391  1402.109985  \n",
       "2000-01-06  3340.810059  11253.259766  1403.449951  \n",
       "2000-01-07  3529.600098  11522.559570  1441.469971  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ibm/full_ibm.csv')\n",
    "df['Date.1'] = pd.to_datetime(df['Date.1'])\n",
    "df.set_index('Date.1', inplace=True)\n",
    "df.drop(['Date'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## params\n",
    "\n",
    "\n",
    "batch_size = 50 # SGD batch size\n",
    "# n_layers = 2 # Number of LSTM cells for multi LSTM cells\n",
    "temporal_windows = 30\n",
    "n_features = df.shape[1]\n",
    "hidden_size = 150 # lstm weight size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.zeros((df.shape[0]-30, 30, df.shape[1]))\n",
    "df_array = df.as_matrix()\n",
    "for i in range(x.shape[0]):\n",
    "    x[i,:,:] = df_array[i:30+i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>NDX</th>\n",
       "      <th>DJIA</th>\n",
       "      <th>SP500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date.1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-23</th>\n",
       "      <td>167.000000</td>\n",
       "      <td>167.490005</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>166.710007</td>\n",
       "      <td>1701200</td>\n",
       "      <td>165.402189</td>\n",
       "      <td>4940.020020</td>\n",
       "      <td>19933.810547</td>\n",
       "      <td>2263.790039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>166.979996</td>\n",
       "      <td>167.979996</td>\n",
       "      <td>166.850006</td>\n",
       "      <td>167.139999</td>\n",
       "      <td>1397500</td>\n",
       "      <td>165.828809</td>\n",
       "      <td>4965.810059</td>\n",
       "      <td>19945.039062</td>\n",
       "      <td>2268.879883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>167.289993</td>\n",
       "      <td>167.740005</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.190002</td>\n",
       "      <td>1757500</td>\n",
       "      <td>164.886264</td>\n",
       "      <td>4926.290039</td>\n",
       "      <td>19833.679688</td>\n",
       "      <td>2249.919922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>166.020004</td>\n",
       "      <td>166.990005</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>166.600006</td>\n",
       "      <td>1663500</td>\n",
       "      <td>165.293051</td>\n",
       "      <td>4918.279785</td>\n",
       "      <td>19819.779297</td>\n",
       "      <td>2249.260010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>166.440002</td>\n",
       "      <td>166.699997</td>\n",
       "      <td>165.500000</td>\n",
       "      <td>165.990005</td>\n",
       "      <td>2952800</td>\n",
       "      <td>164.687836</td>\n",
       "      <td>4863.620117</td>\n",
       "      <td>19762.599609</td>\n",
       "      <td>2238.830078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Volume  \\\n",
       "Date.1                                                                \n",
       "2016-12-23  167.000000  167.490005  166.449997  166.710007  1701200   \n",
       "2016-12-27  166.979996  167.979996  166.850006  167.139999  1397500   \n",
       "2016-12-28  167.289993  167.740005  166.000000  166.190002  1757500   \n",
       "2016-12-29  166.020004  166.990005  166.000000  166.600006  1663500   \n",
       "2016-12-30  166.440002  166.699997  165.500000  165.990005  2952800   \n",
       "\n",
       "             Adj Close          NDX          DJIA        SP500  \n",
       "Date.1                                                          \n",
       "2016-12-23  165.402189  4940.020020  19933.810547  2263.790039  \n",
       "2016-12-27  165.828809  4965.810059  19945.039062  2268.879883  \n",
       "2016-12-28  164.886264  4926.290039  19833.679688  2249.919922  \n",
       "2016-12-29  165.293051  4918.279785  19819.779297  2249.260010  \n",
       "2016-12-30  164.687836  4863.620117  19762.599609  2238.830078  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.66020004e+02,   1.66990005e+02,   1.66000000e+02,\n",
       "         1.66600006e+02,   1.66350000e+06,   1.65293051e+02,\n",
       "         4.91827978e+03,   1.98197793e+04,   2.24926001e+03])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1,29,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## helper function creating layers\n",
    "\n",
    "def new_weights(shape, stddev):\n",
    "    ## Xavier intialization\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=stddev,dtype=tf.float32)\n",
    "    \n",
    "    return tf.Variable(initial)\n",
    "## Biases initialization\n",
    "def new_biases(length):\n",
    "    initial = tf.constant(value=0, shape=[length], dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def new_layer(in_size, out_size):\n",
    "    stddev = np.sqrt(np.float(2)/(in_size + out_size))\n",
    "    weights = new_weights([in_size, out_size], stddev)\n",
    "    biases = new_biases(out_size)\n",
    "    return {'weights': weights, 'biases':biases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 30, 9), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Placeholder variables to hold input data\n",
    "X = tf.placeholder(tf.float32, [None, temporal_windows, n_features]) # Input data\n",
    "Y = tf.placeholder(tf.float32, [None, 1]) # Labels\n",
    "print (X)\n",
    "\n",
    "print (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Layer: {'biases': <tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref>, 'weights': <tf.Variable 'Variable:0' shape=(150, 1) dtype=float32_ref>}\n",
      "* single cell: <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x0000023E5A033860>\n",
      "* outputs transpose: Tensor(\"rnn/transpose:0\", shape=(?, 30, 150), dtype=float32)\n",
      "* outputs: Tensor(\"transpose_1:0\", shape=(30, ?, 150), dtype=float32)\n",
      "* last_step: Tensor(\"Gather:0\", shape=(?, 150), dtype=float32)\n",
      "* input_regression: Tensor(\"add:0\", shape=(?, 1), dtype=float32)\n",
      "* y_rnn_regression: Tensor(\"Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "layer = new_layer(hidden_size, 1) # output layer, outmost layer of the network\n",
    "print ('* Layer: {0}'.format(layer))\n",
    "\n",
    "# hidden_1_layer = new_layer(hidden_size, 100) # Hidden layer 1, outmost layer of the network\n",
    "# print ('* Hidden Layer 1: {0}'.format(hidden_1_layer))\n",
    "\n",
    "# softmax_layer = new_layer(100, N_LABELS) # Softmax layer, outmost layer of the network\n",
    "# print ('* Hidden Layer 1: {0}'.format(softmax_layer))\n",
    "\n",
    "\n",
    "# 1 cell LSTM\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(hidden_size, state_is_tuple=True) # A single LSTM cell\n",
    "\n",
    "# # Multi LSTM cells\n",
    "# rnn_cells = tf.contrib.rnn.MultiRNNCell([cell] * n_layers)\n",
    "# print ('* rnn_cells: {0}'.format(rnn_cells))\n",
    "# outputs_T, states = tf.contrib.dynamic_rnn(rnn_cells, X_rnn, dtype=tf.float32)\n",
    "# print ('* output transpose: {0}'.format(outputs_T))\n",
    "\n",
    "# initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "print ('* single cell: {0}'.format(cell))\n",
    "## Single rnn cell\n",
    "\n",
    "outputs_T, states = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)\n",
    "print ('* outputs transpose: {0}'.format(outputs_T))\n",
    "\n",
    "outputs = tf.transpose(outputs_T, [1,0,2])\n",
    "print ('* outputs: {0}'.format(outputs))\n",
    "\n",
    "# Use output of last step as input for softmax layer\n",
    "last_step = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "print ('* last_step: {0}'.format(last_step))\n",
    "# input_softmax = tf.matmul(last_step, layer['weights']) + layer['biases']\n",
    "# print ('* input_softmax: {0}'.format(input_softmax))\n",
    "# y_rnn_softmax = tf.nn.softmax(input_softmax)\n",
    "# print ('* y_rnn_softmax: {0}'.format(y_rnn_softmax))\n",
    "\n",
    "\n",
    "## Use mean of output of all steps as input for softmax layer\n",
    "# mean_step = tf.reduce_mean(input_tensor=outputs, axis=0)\n",
    "# print ('* mean_step: {0}'.format(mean_step))\n",
    "\n",
    "# # Hidden layer 1:\n",
    "\n",
    "# hidden_1 = tf.nn.relu(tf.matmul(mean_step, hidden_1_layer['weights']) + hidden_1_layer['biases'])\n",
    "# print hidden_1\n",
    "# input_softmax = tf.matmul(hidden_1, softmax_layer['weights']) + softmax_layer['biases']\n",
    "\n",
    "input_regression = tf.matmul(last_step, layer['weights']) + layer['biases']\n",
    "print ('* input_regression: {0}'.format(input_regression))\n",
    "y_rnn_regression = tf.nn.sigmoid(input_regression)\n",
    "print ('* y_rnn_regression: {0}'.format(y_rnn_regression))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Khanh\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "cost = tf.reduce_mean(tf.square(y_rnn_regression-Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost)\n",
    "## Making Prediction\n",
    "y_pred = y_rnn_regression\n",
    "y_true = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "TRAINING_EPOCHS = 20000\n",
    "\n",
    "## Helper function for optimization\n",
    "def optimize(train_x, train_y, n_epochs, batch_size, session, saver, dir_path):\n",
    "    n_samples = train_x.shape[0]\n",
    "    n_iterations = np.int(np.floor(n_samples/batch_size))+1\n",
    "    start_time = time.time()\n",
    "    cost_history = np.empty(shape=[1],dtype=float)\n",
    "    print (\"Training.......\")\n",
    "    if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "    for epoch in np.arange(n_epochs+1):\n",
    "        for itr in np.arange(n_iterations):\n",
    "            start = (itr * batch_size) % (n_samples - batch_size)\n",
    "            batch_x, batch_y = train_x[start:start + batch_size], train_y[start:start + batch_size]\n",
    "            feed_dict_train = {X: batch_x, Y: batch_y}\n",
    "            _, c = session.run([optimizer, cost], feed_dict=feed_dict_train)\n",
    "            cost_history = np.append(cost_history,c)\n",
    "            \n",
    "        if(epoch % 1000 == 0):\n",
    "            print (\"-- Elapsed time -- Epoch -- Cost value -- \")\n",
    "            print (\"-- {:12.6f} -- {:5d} -- {:10.5f} -- \".format((time.time() - start_time), \n",
    "                                                                                    epoch, \n",
    "                                                                                    c, \n",
    "                                                                                    ))\n",
    "            print (\"-- Making prediction at {}th epoch\".format(epoch))\n",
    "            make_prediction(x_test, df_target.loc['2014-01-01':], sess, BATCH_SIZE)\n",
    "            plt.savefig('{0}{1}th_epoch'.format(dir_path, epoch), dpi=1000)\n",
    "        \n",
    "#             Draw weights of convolutional layer\n",
    "#             if(epoch % (n_epochs/2) == 0):\n",
    "#                 plot_conv_weights(session, conv_weights[0], 'conv_1', 1, epoch)\n",
    "#                 plot_conv_weights(session, conv_weights[1], 'conv_2', 1, epoch)\n",
    "#                 plot_conv_weights(session, conv_weights[2], 'conv_3', 1, epoch)\n",
    "                  \n",
    "#         Save model in folder rnn_model\n",
    "        \n",
    "#         saver.save(sess, 'rnn_model/new_cnn')\n",
    "        \n",
    "#         print running time and output cost value graph\n",
    "    print (\"---Total Running time: %s seconds ---\" % (time.time() - start_time))\n",
    "    print ('*'*50)\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.clf()\n",
    "    plt.plot(cost_history)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Helper function to print confusion matrix\n",
    "def make_prediction(test_x, test_y, session, batch_size):\n",
    "    print (\"Making prediction.......\")\n",
    "    start_time = time.time()\n",
    "    n_samples = test_x.shape[0]\n",
    "    n_iterations = np.int(np.floor(n_samples/batch_size))+1\n",
    "    pred = np.zeros((n_samples, test_y.shape[1]))\n",
    "    true = test_y.as_matrix()\n",
    "    for itr in np.arange(n_iterations):\n",
    "        start = (itr * batch_size) % (n_samples - batch_size)\n",
    "        batch_x = test_x[start:start + batch_size]\n",
    "        feed_dict_test = {X: batch_x}\n",
    "        pred[start:start + batch_size] = session.run(y_pred, feed_dict=feed_dict_test)\n",
    "    rmse = math.sqrt(metrics.mean_squared_error(pred, true))\n",
    "    mae = metrics.mean_absolute_error(pred, true)\n",
    "    print (\"RMSE = {}\".format(rmse))\n",
    "    print (\"MAE = {}\".format(mae))\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.plot(test_y.index, pred[:,0], color = 'r', label='Predicted Values')\n",
    "    ax.plot(test_y.index, true[:,0], color = 'b', label='Acutal Values')\n",
    "    ax.legend(loc='upper right', shadow=False)\n",
    "#     plt.show()\n",
    "    print (\"---Running Time: {0} seconds ---\".format((time.time() - start_time)))\n",
    "    print ('*'*50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
